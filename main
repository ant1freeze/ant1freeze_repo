# -*- coding: utf8 -*-
import requests
import re
from bs4 import BeautifulSoup
import csv
from user_agent import generate_user_agent
import random
import time
from multiprocessing import Pool
import pandas as pd


headers = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.35 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.35',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36',
        'Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML like Gecko) Chrome/44.0.2403.155 Safari/537.36',
        'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.1 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.157 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36',
        'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36']

def get_html(barcode):
    header = {'User-Agent': random.choice(headers)}
    # r = requests.head(url, headers = headers)
    # print(r)
    url = 'https://foodsciffer.org/api/search?T='+str(barcode)
    r = requests.get(url, headers=header)

    return r.text


def get_all_barcodes(file):
    links = []
    for i in file.index:
        barcode = file.iloc[i, 1]
        #print(barcode)
        links.append(barcode)
    return links

def get_page_data(html):
    soup = BeautifulSoup(html, 'lxml')
    try:
        name = soup.find('h1', class_ = 'small-space').text.strip()
    except:
        name = ''

    try:
        price = soup.find('span', id='product-price').text.strip().replace(' ', '')
    except:
        price = ''
    try:
        short_descr_ps = soup.find('div', itemprop='description').find_all('p', class_ ='lead')
    except:
        short_descr_ps = ''
    try:
        short_descr_uls = soup.find('div', itemprop='description').find_all('ul')#.text#.find_all('li').strip()
    except:
        short_descr_uls = ''
    try:
        image_divs = soup.find('div', class_='product-images').find_all('a')
    except:
        image_divs = ''
    try:
        thumbnail_divs = soup.find('div', class_='product-thumbnails').find_all('a')
    except:
        thumbnail_divs = ''

        #name=price=price_diler=count=short_descr=images=category_1=category_2=category_3=category_4=descr = ''

    data = {'Banner_name': Banner_name,
            'Barcode': Barcode,
            'Name': Name,
            'grams_of_sugar': price_diler}

    return data

def write_csv(data):
    with open('F:\Download\sugar.csv', 'a', encoding='utf8', errors='ignore', newline='') as f:
        writer = csv.writer(f, delimiter=';')
        writer.writerow((data['Banner_name'],
                         data['Barcode'],
                         data['Name'],
                         data['grams_of_sugar']))

# def make_all(url):
#     #print(url)
#     html = get_html(url)
#     price_diler = get_all_barcodes(pd.read_excel (r'F:\Download\sugar.xlsx'))[url]
#     data = get_page_data(html, price_diler, url)
#     write_csv(data)

def main():
    #file = pd.read_excel (r'C:\Users\alexa\PycharmProjects\untitled\DJI ПРАЙС Дилерский 23.05.2019.xls')
    all_barcodes = get_all_barcodes(pd.read_excel (r'F:\Download\sugar.xlsx'))
    print(len(all_barcodes))
    with open('F:\Download\sugar.csv', 'a', encoding='utf8', errors='ignore', newline='') as f:
        writer = csv.writer(f, delimiter=';')
        writer.writerow(('Banner_name', 'Barcode', 'Name', 'grams of sugar per 100 gr'))
    for barcode in all_barcodes:
        print(barcode)
        # barcode = 'https://foodsciffer.org/api/search?T=' + str(barcode)
        # print(barcode)
        html = get_html(barcode)
        #data = get_page_data(html)
        print(html)
        # write_csv(data)
        # time.sleep(random.choice([1, 2]))
    # with Pool(5) as p:
    #     p.map(make_all, all_links)

if __name__ == '__main__':
    main()


